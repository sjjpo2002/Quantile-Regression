{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantile Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<I>Quantile regression is a type of regression analysis used in statistics and econometrics. Whereas the method of least squares results in estimates that approximate the conditional mean of the response variable given certain values of the predictor variables, quantile regression aims at estimating either the conditional median or other quantiles of the response variable</I>. ([Wikipedia](https://en.wikipedia.org/wiki/Quantile_regression))\n",
    "\n",
    "The most popular form of this regression is where we aim for median or 50th quantile. The problem then boils down to a convex optimization problem with the cost function in the form of [Least absolute deviations](https://en.wikipedia.org/wiki/Least_absolute_deviations). I came across a question lately, asking to solve this problem for a special case of a linear function without any bias (fixed term). The question itself and the solution are discussed here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B> Given two vectors X and Y in $R^n$, please implement an algorithm to compute a scalar beta that minimizes \n",
    " \n",
    " $\\| beta * X - Y \\|_1    $\n",
    " \n",
    " and explain its complexity. </B>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a piecewise-linear optimization problem that arises in regression modeling, signal processing and some other applications. This optimization problem can be solved by Linear Programming methods. But since the question asks for not using any solver or known methods we will use simple properties of convex piecewise-linear nature of the problem to find the minimum. Here are the highlights of the approach used to solve this problem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The calculations are done in Python 2.7 using numpy package\n",
    "* This problem doesn't have unique solution in some scenarios but the minimum norm can be found\n",
    "* Since we know it is a convex piecewise-linear optimization we know that the minimum happens on Vertices\n",
    "* We also know that for Least Absolute Deviations problem the answer converges to the median of roots (Vertices)\n",
    "* The algorithm proposed calculates all the roots (Vertices) and finds the Vertex for which norm is the lowest\n",
    "* Only vertices around median are processed (2 vertices if $n$ is even, 3 vertices if $n$ is odd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complexity of this algorithm is $nlog(n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because we have to sort the $n$ vertices to find the median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from numpy.testing import assert_allclose\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def around_median(samples):\n",
    "    samples = np.unique(samples) # sort unique elements\n",
    "    length = len(samples)\n",
    "    if length < 4:\n",
    "        return samples\n",
    "    else:\n",
    "        if length % 2 == 0:  # even \n",
    "            return samples[length/2-1:length/2+1]\n",
    "        else: # odd\n",
    "            return samples[length/2-1:length/2+2]\n",
    "\n",
    "def find_min(X, Y):\n",
    "    current = np.inf\n",
    "    best = None\n",
    "    candidates = around_median(np.divide(Y, X))\n",
    "    for item in candidates:\n",
    "        L1 = norm(item*X-Y, ord=1)\n",
    "        if L1 < current:\n",
    "            best = item\n",
    "            current = L1\n",
    "    return best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Too test the proposed method we run the code for few test cases. If there is a optimum value it will be returned by find_min() otherwise it will return None. Don't throw empty vectors at it cuz None is what you will get!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testcase 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this test we supply uniformly distributed random numbers with $n=11$ as an odd number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Minimum norm is  627.026968432 Which happens for betha =  -0.232565869666\n",
      "\n",
      "PASSED: the answer has less than 1% error\n",
      "627.026968432 is approximately equal to expected 627.026968432\n"
     ]
    }
   ],
   "source": [
    "# Test Case1    ** Odd number of samples **\n",
    "X1 = np.random.uniform(-100.0, 100, 11)\n",
    "Y1 = np.random.uniform(-100.0, 100, 11)\n",
    "optimum = find_min(X1, Y1)\n",
    "lowest_norm = norm(optimum*X1-Y1, ord=1)\n",
    "print \"\\n\\nMinimum norm is \", lowest_norm, \"Which happens for betha = \", optimum\n",
    "try:\n",
    "    expected = min([norm(item*X1-Y1, ord=1) for item in np.divide(Y1, X1)])\n",
    "    assert_allclose(lowest_norm, expected, 0.01)\n",
    "    print \"\\nPASSED: the answer has less than 1% error\"\n",
    "    print \"{} is approximately equal to expected {}\".format(lowest_norm, expected)\n",
    "\n",
    "except AssertionError:\n",
    "    print \"\\nFAILED: the error is more than 1%\"\n",
    "    print \"found {} but was expecting {}\".format(lowest_norm, expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testcase 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this test we supply uniformly distributed random numbers with $n=222$ as an even number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Minimum norm is  1169.09365168 Which happens for betha =  0.216295171636\n",
      "\n",
      "PASSED: the answer has less than 1% error\n",
      "1169.09365168 is approximately equal to expected 1169.09365168\n"
     ]
    }
   ],
   "source": [
    "# Test Case2    ** Even number of samples **\n",
    "X2 = np.random.uniform(-100.0, 100, 22)\n",
    "Y2 = np.random.uniform(-100.0, 100, 22)\n",
    "optimum = find_min(X2, Y2)\n",
    "lowest_norm = norm(optimum*X2-Y2, ord=1)\n",
    "print \"\\n\\nMinimum norm is \", lowest_norm, \"Which happens for betha = \", optimum\n",
    "try:\n",
    "    expected = min([norm(item*X2-Y2, ord=1) for item in np.divide(Y2, X2)])\n",
    "    assert_allclose(lowest_norm, expected, 0.01)\n",
    "    print \"\\nPASSED: the answer has less than 1% error\"\n",
    "    print \"{} is approximately equal to expected {}\".format(lowest_norm, expected)\n",
    "\n",
    "except AssertionError:\n",
    "    print \"\\nFAILED: the error is more than 1%\"\n",
    "    print \"found {} but was expecting {}\".format(lowest_norm, expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testcase 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this test we supply uniformly distributed random numbers with some roots that happen to be the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Minimum norm is  1090.52047116 Which happens for betha =  0.0956563372043\n",
      "\n",
      "PASSED: the answer has less than 1% error\n",
      "1090.52047116 is approximately equal to expected 1090.52047116\n"
     ]
    }
   ],
   "source": [
    "# Test Case3    ** Some roots are equal**\n",
    "X3 = np.random.uniform(-100.0, 100, 22)\n",
    "X3 = np.append(X3, [1, 2])\n",
    "Y3 = np.random.uniform(-100.0, 100, 22)\n",
    "Y3 = np.append(Y3, [3, 6])\n",
    "optimum = find_min(X3, Y3)\n",
    "lowest_norm = norm(optimum*X3-Y3, ord=1)\n",
    "print \"\\n\\nMinimum norm is \", lowest_norm, \"Which happens for betha = \", optimum\n",
    "try:\n",
    "    expected = min([norm(item*X3-Y3, ord=1) for item in np.divide(Y3, X3)])\n",
    "    assert_allclose(lowest_norm, expected, 0.01)\n",
    "    print \"\\nPASSED: the answer has less than 1% error\"\n",
    "    print \"{} is approximately equal to expected {}\".format(lowest_norm, expected)\n",
    "\n",
    "except AssertionError:\n",
    "    print \"\\nFAILED: the error is more than 1%\"\n",
    "    print \"found {} but was expecting {}\".format(lowest_norm, expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testcase 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this test we try to see if we can survive Zero division. I used numpy divide method to make sure it will handle this. Let's see what happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Minimum norm is  2356.87297815 Which happens for betha =  -0.0318223666051\n",
      "\n",
      "PASSED: the answer has less than 1% error\n",
      "2356.87297815 is approximately equal to expected 2340.0623818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python27\\lib\\site-packages\\ipykernel\\__main__.py:15: RuntimeWarning: divide by zero encountered in divide\n",
      "c:\\python27\\lib\\site-packages\\ipykernel\\__main__.py:9: RuntimeWarning: divide by zero encountered in divide\n",
      "c:\\python27\\lib\\site-packages\\ipykernel\\__main__.py:9: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    }
   ],
   "source": [
    "# Test Case4    ** Zero Division **\n",
    "X4 = np.random.uniform(-100.0, 100, 44)\n",
    "X4 = np.append(X4, [0.0, 0.0])\n",
    "Y4 = np.random.uniform(-100.0, 100, 46)\n",
    "optimum = find_min(X4, Y4)\n",
    "lowest_norm = norm(optimum*X4-Y4, ord=1)\n",
    "print \"\\n\\nMinimum norm is \", lowest_norm, \"Which happens for betha = \", optimum\n",
    "try:\n",
    "    expected = min([norm(item*X4-Y4, ord=1) for item in np.divide(Y4, X4)])\n",
    "    assert_allclose(lowest_norm, expected, 0.01)\n",
    "    print \"\\nPASSED: the answer has less than 1% error\"\n",
    "    print \"{} is approximately equal to expected {}\".format(lowest_norm, expected)\n",
    "\n",
    "except AssertionError:\n",
    "    print \"\\nFAILED: the error is more than 1%\"\n",
    "    print \"found {} but was expecting {}\".format(lowest_norm, expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got the warning but it's nothing to worry about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might wonder what if we just find the derivitave of the function with respect to our independant variable and make it equal to zero. Let's call this Sardar's method (the name of the person who asked this question!). This method has the advantage of lower complexity (order n). But it doesn't converge to the answer very accurately. This can be explained with the fact that the function doesn't have derivitave in the vertices. We will plot the function to show what it means at the end. A few examples using this method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array([-1.0, 2.0, 3.0])\n",
    "Y = np.array([4, 5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.0, 7.0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_beta = find_min(X, Y)\n",
    "lowest_norm = norm(opt_beta*X-Y, ord=1)\n",
    "opt_beta, lowest_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.7142857142857142, 8.1428571428571441)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sardar_beta = sum(Y*X)/sum(X**2)\n",
    "sardar_norm = norm(sardar_beta*X-Y, ord=1)\n",
    "sardar_beta, sardar_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "betas = np.arange(-2, 5, 0.01)\n",
    "norms = []\n",
    "for item in betas:\n",
    "    norms.append(norm(item*X-Y, ord=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0xac66588>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXEAAAEACAYAAABF+UbAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xaabd748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(betas, norms)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
